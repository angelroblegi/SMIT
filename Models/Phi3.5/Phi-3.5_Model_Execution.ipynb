{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# <FONT COLOR=\"RED\">***Overview***</FONT>\n","\n","---\n","---\n"],"metadata":{"id":"iyq8PqNzcrCh"}},{"cell_type":"markdown","source":["DescripciÃ³n de la secciÃ³n."],"metadata":{"id":"305LbbHScz3y"}},{"cell_type":"markdown","source":["# <FONT COLOR=\"ORANGE\">**Prepare Notebook**</FONT>\n","\n","---\n","---\n","\n"],"metadata":{"id":"uX5cgFV5c44V"}},{"cell_type":"markdown","source":["It's important to note that the notebook is designed to run from **Google Colab** (with free GPU T4 resources), so some packages used aren't installed, as they're included by default in the environment. If you're having issues with any of the packages, we recommend checking your installed versions to install the missing packages in their supported versions."],"metadata":{"id":"ynBE9g-hdHAm"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Q4l5w1y2XQWL","executionInfo":{"status":"ok","timestamp":1759462352919,"user_tz":300,"elapsed":20,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"outputs":[],"source":["# Color messages function\n","def print_mess(mess:str, color=\"green\") -> None:\n","  match color:\n","    case \"green\":\n","      print(f\"\\033[92m{mess}\\033[0m\")\n","    case \"red\":\n","      print(f\"\\033[91m{mess}\\033[0m\")\n","    case \"yellow\":\n","      print(f\"\\033[93m{mess}\\033[0m\")\n","    case \"white\":\n","      print(f\"\\033[97m{mess}\\033[0m\")\n"]},{"cell_type":"code","source":["# Identify environment keys\n","import os\n","import subprocess\n","\n","# Google Colaboratory environment flag\n","is_in_Google_Colab = True\n","\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","  is_in_Google_Colab = False\n","  print_mess(\"Notebook is actually running out of Google Colaboratory environment\", \"yellow\")\n","  print_mess(\"Installing necessary packages...\", \"white\")\n","  try:\n","    # Install unsloth first with its dependencies\n","    subprocess.run([\n","        \"pip\", \"install\",\n","        \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","    ])\n","    subprocess.run([\n","        \"pip\", \"install\", \"--no-deps\"\n","        \"xformers\", \"trl<=0.9.0\", \"peft\", \"accelerate\", \"bitsandbytes\"\n","    ])\n","    subprocess.run([\n","        \"pip\", \"install\",\n","        \"datasets>=3.4.1,<4.0.0\", \"tqdm\",\n","    ])\n","    print_mess(\"Packages installed successfully\", \"green\")\n","  except Exception as e:\n","    print_mess(\"Something went wrong\", \"red\")\n","else:\n","  print_mess(\"Notebook is running in Google Colaboratory environment\", \"green\")\n","  print_mess(\"Installing necessary packages...\", \"white\")\n","  try:\n","    # Install unsloth first with its dependencies\n","    subprocess.run([\n","        \"pip\", \"install\",\n","        \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","    ])\n","    subprocess.run([\n","        \"pip\", \"install\", \"--no-deps\"\n","        \"xformers==0.0.29.post3\", \"trl<=0.9.0\", \"peft\", \"accelerate\", \"bitsandbytes\"\n","    ])\n","\n","    # Install other packages\n","    subprocess.run([\n","        \"pip\", \"install\", \"--upgrade\"\n","        \"google-api-python-client\", \"google-auth-httplib2\", \"google-auth-oauthlib\"\n","    ])\n","\n","    subprocess.run([\n","        \"pip\", \"install\",\n","        \"datasets>=3.4.1,<4.0.0\", \"tqdm\"\n","    ])\n","\n","    subprocess.run([\n","        \"pip\", \"install\",\n","        \"sentencepiece\", \"protobuf\", \"huggingface_hub\", \"hf_transfer\"\n","    ])\n","\n","    print_mess(\"Packages installed successfully\", \"green\")\n","  except Exception as e:\n","    print_mess(\"Something went wrong\", \"red\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rAVjgUjcgbp8","executionInfo":{"status":"ok","timestamp":1759462398675,"user_tz":300,"elapsed":45751,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}},"outputId":"dc943541-ebfc-41c1-dc74-92ca374ae9bb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92mNotebook is running in Google Colaboratory environment\u001b[0m\n","\u001b[97mInstalling necessary packages...\u001b[0m\n","\u001b[92mPackages installed successfully\u001b[0m\n"]}]},{"cell_type":"code","source":["# Fine-tune packages\n","from unsloth import FastLanguageModel, is_bfloat16_supported, unsloth_train # This import need ve first\n","import numpy as np\n","import time\n","import traceback\n","import torch\n","from transformers import TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback\n","from trl import SFTTrainer, SFTConfig\n","\n","# Sava model packages\n","from datetime import datetime\n","\n","# Nootebook visual presentation packages\n","from tqdm import tqdm\n","\n","# Dataset packages\n","import io\n","import json\n","from datasets import Dataset\n","from typing import Union\n","import re\n","from unsloth.chat_templates import get_chat_template\n","\n","# Google authentication packages only used in Google Colaboratory environment\n","if is_in_Google_Colab:\n","  from google.colab import auth\n","  from googleapiclient.errors import HttpError\n","  from googleapiclient.discovery import build\n","  from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload, MediaFileUpload"],"metadata":{"id":"fwMfezKDggld","executionInfo":{"status":"error","timestamp":1759462431541,"user_tz":300,"elapsed":32855,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}},"colab":{"base_uri":"https://localhost:8080/","height":633},"outputId":"466a2353-3504-407c-8834-8b8c9a3fcba3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]},{"output_type":"error","ename":"ImportError","evalue":"Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0munsloth_zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m from .temporary_patches import (\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mencode_conversations_with_harmony\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/temporary_patches/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/temporary_patches/gemma.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTEMPORARY_PATCHES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mpatch_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/temporary_patches/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrender_jinja_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msun397\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSUN397\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msvhn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVHN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mucf101\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUCF101\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-983755068.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fine-tune packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_bfloat16_supported\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsloth_train\u001b[0m \u001b[0;31m# This import need ve first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0munsloth_zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: Unsloth: Please install unsloth_zoo via `pip install unsloth_zoo`","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["If you're using **Google Collaboratory** to run this notebook and don't have the model files, we recommend mounting the **Google Drive** through the authentication cell.\n","\n","If you **don't want to allow access to your Google Drive**, we invite you to visit our **[GitHub repository](https://)** where you can find the notebook that train the model and there you can save the model files in your local storage."],"metadata":{"id":"dlB62XUQfo5_"}},{"cell_type":"code","source":["# Google connection and authentication\n","if is_in_Google_Colab:\n","  auth.authenticate_user()\n","  drive_service = build(\"drive\", \"v3\")"],"metadata":{"id":"XoANqPCvf4bn","executionInfo":{"status":"aborted","timestamp":1759462431628,"user_tz":300,"elapsed":149,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Likewise, if you're working in an **environment outside of Google Colaboratory** or have chosen **not to mount your Google Drive** to the Google Colaboratory environment, you must have the model files in the same directory for use with this notebook."],"metadata":{"id":"Q9M6-asRrXsx"}},{"cell_type":"code","source":["# Here you must put the name of your folder name that allocated the model files\n","main_model_folder_name = \"Model_Versions\"\n","\n","# Here you must put the name of your files with the JSON extension when notebook\n","# is outside of Google Colaboratory\n","eval_dataset_name = \"RMI_Intentions_Evaluation_Dataset.json\""],"metadata":{"id":"HGjm1YDpf6cj","executionInfo":{"status":"aborted","timestamp":1759462431633,"user_tz":300,"elapsed":131,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Search folder function\n","def search_folder(id_folder:str, name_folder:str) -> dict:\n","  folder_list = drive_service.files().list(\n","      q=f\"'{id_folder}' in parents and mimeType = 'application/vnd.google-apps.folder' and name contains '{name_folder}'\",\n","      spaces=\"drive\",\n","      fields=\"nextPageToken, files(id, name, createdTime)\",\n","      orderBy=\"createdTime desc\"\n","  ).execute()\n","\n","  return folder_list"],"metadata":{"id":"nXWpMSwLHVAz","executionInfo":{"status":"aborted","timestamp":1759462431636,"user_tz":300,"elapsed":97,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If notebook is executed from Google Colaboratory, and you want to use the last\n","# version of the model trained files, it's necessary that mount the Google Drive\n","# unit and execute the following cells.\n","\n","if is_in_Google_Colab:\n","  main_model_folder_id = \"1giaO-m_gID4LImM1x4AoyebrsfW6DLYY\"\n","  main_model_folder_name = \"Translation_Intent_Model_\"\n","  main_dataset_folder_id = \"1liZABLNR-wodEouxbAaqTvYdrtkOKuKx\"\n","  main_dataset_folder_name = \"RMI_Intentions_Dataset_\"\n","\n","  # Search the most recent created folder\n","  model_folder_list = search_folder(main_model_folder_id, main_model_folder_name)\n","\n","  # Search the most recent created folder\n","  dataset_folder_list = search_folder(main_dataset_folder_id, main_dataset_folder_name)\n","\n","  # Obtain the most recent folder id of the model folder\n","  if not model_folder_list.get(\"files\", []):\n","    print_mess(\"No folders found\", \"red\")\n","  else:\n","    model_folder_id = model_folder_list.get(\"files\", [])[0][\"id\"]\n","    print_mess(f\"The most recent model folder ID is: {model_folder_id}\")\n","\n","  # Obtain the most recent folder id of the dataset folder\n","  if not dataset_folder_list.get(\"files\", []):\n","    print_mess(\"No folders found\", \"red\")\n","  else:\n","    dataset_folder_id = dataset_folder_list.get(\"files\", [])[0][\"id\"]\n","    print_mess(f\"The most recent dataset folder ID is: {dataset_folder_id}\")"],"metadata":{"id":"b0rakg5-ry4V","executionInfo":{"status":"aborted","timestamp":1759462431671,"user_tz":300,"elapsed":40,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the local directory to download the model files if it doesn't exist\n","local_download_path = \"/content/Q-LoRa_Phi_3_Model\"\n","os.makedirs(local_download_path, exist_ok=True)"],"metadata":{"id":"1Zm35M3I2xue","executionInfo":{"status":"aborted","timestamp":1759462431691,"user_tz":300,"elapsed":2,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Search and save the model files\n","if is_in_Google_Colab:\n","  id_model_file_found = []\n","  name_model_file_found = []\n","\n","  model_file_list = drive_service.files().list(\n","      q = f\"'{model_folder_id}' in parents\",\n","      spaces = \"drive\",\n","      fields = \"nextPageToken, files(id, name)\"\n","  ).execute()\n","\n","  # Identify file in the file list\n","  if not model_file_list.get(\"files\", []):\n","    print_mess(\"No files found\", \"red\")\n","  else:\n","    id_model_file_found = [file[\"id\"] for file in model_file_list.get(\"files\", [])]\n","    name_model_file_found = [file[\"name\"] for file in model_file_list.get(\"files\", [])]\n","    for i in range(len(id_model_file_found)):\n","      print_mess(f\"The file '{name_model_file_found[i]}' was found with ID: {id_model_file_found[i]}\")"],"metadata":{"id":"2N4zcn_V2zfu","executionInfo":{"status":"aborted","timestamp":1759462431694,"user_tz":300,"elapsed":79052,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download function\n","def download_and_save_file(file_id:str, file_name:str, local_path:str, model_download:bool) -> Union[str, None]:\n","  # Request to download file\n","  request=drive_service.files().get_media(fileId=file_id)\n","  fh = io.BytesIO()\n","  downloader = MediaIoBaseDownload(fh, request)\n","  done = False\n","\n","  # Download process\n","  with tqdm(total = 100, unit = \"%\", desc = f\"Downloading {file_name} file\") as pbar:\n","    while done is False:\n","      status, done = downloader.next_chunk()\n","      pbar.update(int(status.progress() * 100) - pbar.n)\n","\n","  if model_download:\n","    # Save process\n","    with open(os.path.join(local_path, file_name), \"wb\") as f:\n","      f.write(fh.getvalue())\n","    return None\n","  else:\n","    # Move the cursor to the beginning for correct reading of the file\n","    fh.seek(0)\n","    return fh.read().decode(\"utf-8\")"],"metadata":{"id":"CftmuQkj5Wk6","executionInfo":{"status":"aborted","timestamp":1759462431701,"user_tz":300,"elapsed":79058,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download model files in the Google Colab environment\n","if is_in_Google_Colab:\n","  try:\n","    print_mess(\"Downloading model files...\", \"green\")\n","    for i in range(len(id_model_file_found)):\n","      print_mess(f\"Downloading {name_model_file_found[i]}...\", \"white\")\n","      download_and_save_file(id_model_file_found[i], name_model_file_found[i], local_download_path, True)\n","    print_mess(\"\\nModel files downloaded successfully\", \"green\")\n","  except Exception as e:\n","    print_mess(\"Something went wrong\", \"red\")\n","else:\n","  try:\n","    print_mess(\"Reading model files...\", \"green\")\n","    for i in range(len(id_model_file_found)):\n","      print_mess(f\"Reading {name_model_file_found[i]}...\", \"white\")\n","      with open(name_model_file_found[i], \"r\") as f:\n","        file_content = f.read()\n","        with open(os.path.join(local_download_path, name_model_file_found[i]), \"w\") as outfile:\n","          outfile.write(file_content)\n","      print_mess(f\"{name_model_file_found[i]} saved\", \"white\")\n","    print_mess(\"Model files read successfully\", \"green\")\n","  except Exception as e:\n","    print_mess(\"Something went wrong\", \"red\")"],"metadata":{"id":"h0zwmtCB5eeq","executionInfo":{"status":"aborted","timestamp":1759462431705,"user_tz":300,"elapsed":79057,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Likewise, if you're working in an **environment outside of Google Colaboratory** or have chosen **not to mount your Google Drive** to the Google Colaboratory environment, you must have the evaluation dataset file in the same directory for use with this notebook."],"metadata":{"id":"47UWhz8fFm8n"}},{"cell_type":"code","source":["# Find datasets inside the Google Drive folder\n","if is_in_Google_Colab:\n","  dataset_list = drive_service.files().list(\n","      q = f\"'{dataset_folder_id}' in parents and name = '{eval_dataset_name}'\",\n","      spaces = \"drive\",\n","      fields = \"nextPageToken, files(id, name)\"\n","  ).execute()\n","\n","  # Identify item in the result file list\n","  if not dataset_list.get(\"files\", []):\n","    print_mess(f\"The file '{eval_dataset_name}' was not found in the provided folder\", \"red\")\n","  else:\n","    id_evaluation_dataset = dataset_list.get(\"files\", [])[0][\"id\"]\n","    print_mess(f\"The file '{eval_dataset_name}' was found with ID: {id_evaluation_dataset}\")"],"metadata":{"id":"r2A2KisFNuBt","executionInfo":{"status":"aborted","timestamp":1759462431713,"user_tz":300,"elapsed":79059,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download txt file in the Google Colaboratoy environment\n","if is_in_Google_Colab:\n","  try:\n","    print_mess(\"Downloading evaluation datasets...\", \"green\")\n","    eval_json_dataset = json.loads(download_and_save_file(id_evaluation_dataset, eval_dataset_name, \"\", False))\n","    print_mess(\"\\nDataset downloaded successfully\", \"green\")\n","  except Exception as e:\n","    print_mess(\"Something went wrong\", \"red\")\n","    print(e)\n","else:\n","  try:\n","    print_mess(\"Reading evaluation datasets...\", \"green\")\n","    with open(eval_dataset_name, \"r\") as f:\n","      eval_json_dataset = json.loads(f.read())\n","    print_mess(\"Dataset read successfully\", \"green\")\n","  except Exception as e:\n","    print_mess(\"Something went wrong\", \"red\")"],"metadata":{"id":"Zg1a-uYMNJ3Z","executionInfo":{"status":"aborted","timestamp":1759462431718,"user_tz":300,"elapsed":79058,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <FONT COLOR=\"ORANGE\">**Download Trained Model**</FONT>\n","\n","---\n","---\n"],"metadata":{"id":"oCs6V_y5-7ts"}},{"cell_type":"markdown","source":["For this project, **we chose to use the Phi-3 model**, specifically its **small language model (SLM) version called \"Phi-3-mini-4k-instruct\"**, which offers great capabilities as a conversational model and in code generation while consuming minimal resources.\n","\n","In this section, the previously trained and downloaded model is loaded into the Google Colab environment."],"metadata":{"id":"R_Uy520k_Bgs"}},{"cell_type":"code","source":["# Download Model Parameters\n","model_name = \"Q-LoRa_Phi_3_Model\"\n","sequence_length = 4096\n","quantization_model = True\n","seed = 3407"],"metadata":{"id":"_RaztR7y_ATC","executionInfo":{"status":"aborted","timestamp":1759462431721,"user_tz":300,"elapsed":79060,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download Model\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = model_name,\n","    max_seq_length=sequence_length,\n","    dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16,\n","    load_in_4bit = quantization_model\n",")\n","\n","FastLanguageModel.for_inference(model)"],"metadata":{"id":"SuctPNWaAUTW","executionInfo":{"status":"aborted","timestamp":1759462431731,"user_tz":300,"elapsed":79065,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}},"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <FONT COLOR=\"ORANGE\">**Dataset Preprocessing**</FONT>\n","\n","---\n","---\n"],"metadata":{"id":"7w6x6pPKAdEw"}},{"cell_type":"markdown","source":["To further **ensure the model correctly understands the training data**, it is important to provide it with said data in a correct format. In this case, since **Phi-3 is a conversational model that uses special tokens to delimit information**, it is important to make conscious use of these tokens, which are:\n","\n","*   **<|user|>**: Indicates where the input provided by a user begins, i.e., what the model is asked to do.\n","*   **<|assistant|>**: Indicates where the output it should generate as a conversational assistant model begins, i.e., during training, it teaches it how to respond.\n","*   **<|end|>**: Indicates to the model where either of the two previous sections ends. This token is essential for the model to understand where what the user says ends and where the assistant should respond.\n","\n","Seeking to achieve the best possible quality in the responses generated by the model, **we added a custom token to each of the conversations from which the model will learn**, which is:\n","\n","*   **<|system_message|>**: Through this token, we apply the Prompt Engineering Zero-Shot technique during training to further specialize the model's behavior and obtain the most accurate responses possible.\n"],"metadata":{"id":"lOxQ8OQIAg94"}},{"cell_type":"code","source":["# Create prompt template\n","tokenizer = get_chat_template(\n","  tokenizer,\n","  chat_template = 'phi-3',\n","  mapping = {\n","    'role': 'from',\n","    'content': 'value',\n","    'system': 'system_message',\n","    'user': 'user_operator',\n","    'assistant': 'model_answer'\n","  }\n",")"],"metadata":{"id":"UDbA_NG6QDke","executionInfo":{"status":"aborted","timestamp":1759462431734,"user_tz":300,"elapsed":79067,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop the model answer\n","eval_json_dataset_no_answer = eval_json_dataset\n","for conversation in eval_json_dataset_no_answer['conversations']:\n","  for message in conversation:\n","    if message['from'] == 'model_answer':\n","      conversation.remove(message)\n","      break"],"metadata":{"id":"Nhvx541TQruS","executionInfo":{"status":"aborted","timestamp":1759462431737,"user_tz":300,"elapsed":79069,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <FONT COLOR=\"ORANGE\">**Model Execution**</FONT>\n","\n","---\n","---\n"],"metadata":{"id":"ovSl0UuXVYcZ"}},{"cell_type":"markdown","source":["DescripciÃ³n de la secciÃ³n"],"metadata":{"id":"2GLId7W3zfvZ"}},{"cell_type":"code","source":["# Execution Parameters\n","generated_outputs = []\n","generation_times = [] # Lista para almacenar los tiempos de generaciÃ³n\n","\n","for conversation in eval_json_dataset_no_answer[\"conversations\"]:\n","  # Generate tensors\n","  inputs = tokenizer.apply_chat_template(\n","      conversation,\n","      tokenize = True,\n","      add_generation_prompt = True,\n","      return_tensors = \"pt\"\n","  ).to(\"cuda\")\n","\n","  # Start time measurement\n","  start_time = time.time()\n","\n","  # Generate Output\n","  generated_output = model.generate(\n","      input_ids = inputs,\n","      max_new_tokens = sequence_length/2,\n","      use_cache = True\n","  )\n","\n","  # End time measurement and store\n","  end_time = time.time()\n","  generation_times.append(end_time - start_time)\n","\n","  # Decode output to add in list\n","  decoded_output = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n","  generated_outputs.append(decoded_output)\n","\n","print_mess(\"Generation times:\", \"green\")\n","print(generation_times)"],"metadata":{"id":"-Vg1RSayGfRo","executionInfo":{"status":"aborted","timestamp":1759462431748,"user_tz":300,"elapsed":79073,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[0]\n"],"metadata":{"id":"EsN6ti_z8_Wl","executionInfo":{"status":"aborted","timestamp":1759462431752,"user_tz":300,"elapsed":79072,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[1]"],"metadata":{"id":"uFOHLT89p3N2","executionInfo":{"status":"aborted","timestamp":1759462431754,"user_tz":300,"elapsed":79068,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","generated_outputs[2]"],"metadata":{"id":"VlE8gOvxp4H_","executionInfo":{"status":"aborted","timestamp":1759462431757,"user_tz":300,"elapsed":79065,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","generated_outputs[3]"],"metadata":{"id":"iS2E_Qgxp46b","executionInfo":{"status":"aborted","timestamp":1759462431763,"user_tz":300,"elapsed":79066,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[4]"],"metadata":{"id":"KftZ_3gfrrun","executionInfo":{"status":"aborted","timestamp":1759462431768,"user_tz":300,"elapsed":79065,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[5]"],"metadata":{"id":"s3sxEUcqruAm","executionInfo":{"status":"aborted","timestamp":1759462431801,"user_tz":300,"elapsed":79084,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[6]"],"metadata":{"id":"iVNaN5c6ru-s","executionInfo":{"status":"aborted","timestamp":1759462431805,"user_tz":300,"elapsed":79074,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[7]"],"metadata":{"id":"PaVZeB6HrwE5","executionInfo":{"status":"aborted","timestamp":1759462431807,"user_tz":300,"elapsed":79067,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[8]"],"metadata":{"id":"QXbr5E2Iry67","executionInfo":{"status":"aborted","timestamp":1759462431810,"user_tz":300,"elapsed":79058,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[9]"],"metadata":{"id":"ZoTkH1O5r0D-","executionInfo":{"status":"aborted","timestamp":1759462431812,"user_tz":300,"elapsed":79051,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[10]"],"metadata":{"id":"T-KlpAvIr1HZ","executionInfo":{"status":"aborted","timestamp":1759462431814,"user_tz":300,"elapsed":79045,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[11]"],"metadata":{"id":"rfzAX2j9r2So","executionInfo":{"status":"aborted","timestamp":1759462431818,"user_tz":300,"elapsed":79041,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[12]"],"metadata":{"id":"f6tZbOeRr3bR","executionInfo":{"status":"aborted","timestamp":1759462431820,"user_tz":300,"elapsed":79035,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[13]"],"metadata":{"id":"b0ss77qor4eS","executionInfo":{"status":"aborted","timestamp":1759462431823,"user_tz":300,"elapsed":79031,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_outputs[14]"],"metadata":{"id":"p-PIbyz-r51B","executionInfo":{"status":"aborted","timestamp":1759462432096,"user_tz":300,"elapsed":79296,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Result processing\n","tmf_format_outputs = []\n","for output in generated_outputs:\n","  # Regular expression to find text within triple backticks\n","  pattern = re.search(r\"```(.*?)```\", output, re.DOTALL)\n","  if pattern:\n","    # Extract the matched text and remove leading/trailing whitespace\n","    extracted_text = pattern.group(1).strip()\n","    tmf_format_outputs.append(extracted_text)\n","  else:\n","    # If no match is found, It append a empty string to identify errors in generation\n","    tmf_format_outputs.append(\"\")\n","\n","print_mess(\"Results processed successfully\", \"green\")"],"metadata":{"id":"NZAqtSaHWrTP","executionInfo":{"status":"aborted","timestamp":1759462432100,"user_tz":300,"elapsed":79293,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <FONT COLOR=\"ORANGE\">**Save Results**</FONT>\n","\n","---\n","---\n"],"metadata":{"id":"rtuPHO9qbxqt"}},{"cell_type":"code","source":["# Get today's date\n","now = datetime.now()\n","today = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","# Define the folder ID where the new folder will be created\n","folder_id = \"1U9L29UTG59gRVHVy46-gvqhLH5TFjaRf\"\n","\n","# Define the name of the new folder\n","new_folder_name = f\"Model_Results_{today}\""],"metadata":{"id":"lM7252mnb8aR","executionInfo":{"status":"aborted","timestamp":1759462432252,"user_tz":300,"elapsed":79444,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create new folder\n","try:\n","  file_metadata = {\n","      \"name\": new_folder_name,\n","      \"parents\": [folder_id],\n","      \"mimeType\": \"application/vnd.google-apps.folder\"\n","  }\n","  folder = drive_service.files().create(\n","      body = file_metadata,\n","      fields = \"id\"\n","  ).execute()\n","  created_folder_id = folder.get(\"id\")\n","  print_mess(f\"Folder '{new_folder_name}' created successfully with ID: {created_folder_id}\", \"green\")\n","except HttpError as error:\n","  print_mess(f\"An error occurred: {error}\", \"red\")\n","  created_folder_id = None"],"metadata":{"id":"XhcPE4uzRF2R","executionInfo":{"status":"aborted","timestamp":1759462432299,"user_tz":300,"elapsed":79484,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define file name\n","save_file_name = \"generated_tmf_format_outputs.json\"\n","\n","# Convert the list to a JSON string\n","json_generated_outputs = json.dumps(tmf_format_outputs, indent=2)\n","\n","# Create file content as bytes\n","save_file_content = json_generated_outputs.encode(\"utf-8\")"],"metadata":{"id":"SPnGXldgSZXx","executionInfo":{"status":"aborted","timestamp":1759462432410,"user_tz":300,"elapsed":79581,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define save file metadata\n","save_file_metadata = {\n","    \"name\": save_file_name,\n","    \"parents\": [created_folder_id],\n","    \"mimeType\": \"application/json\"\n","}"],"metadata":{"id":"Oj7D3NJlTaDB","executionInfo":{"status":"aborted","timestamp":1759462432412,"user_tz":300,"elapsed":79582,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create MediaIoBaseUpload object\n","media = MediaIoBaseUpload(\n","    io.BytesIO(save_file_content),\n","    mimetype = \"application/json\",\n","    resumable = True\n",")"],"metadata":{"id":"8hxWXUl1ToxZ","executionInfo":{"status":"aborted","timestamp":1759462432413,"user_tz":300,"elapsed":79581,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Upload file\n","try:\n","  uploaded_file = drive_service.files().create(\n","      body = save_file_metadata,\n","      media_body = media,\n","      fields = \"id\"\n","  ).execute()\n","  print_mess(f\"File '{save_file_name}' uploaded successfully with ID: {uploaded_file.get('id')}\", \"green\")\n","except HttpError as error:\n","  print_mess(f\"An error occurred: {error}\", \"red\")"],"metadata":{"id":"xIHL4NbUUFYW","executionInfo":{"status":"aborted","timestamp":1759462432415,"user_tz":300,"elapsed":79575,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# CÃ“DIGO SIMPLIFICADO: CALCULAR MÃ‰TRICAS CARGANDO DESDE GOOGLE DRIVE\n","# ============================================================================\n","\n","import json\n","import numpy as np\n","import subprocess\n","import os\n","import io\n","\n","# Instalar paquetes necesarios para las mÃ©tricas\n","try:\n","    subprocess.run([\"pip\", \"install\", \"scikit-learn\", \"nltk\", \"sacrebleu\", \"matplotlib\"], check=True, capture_output=True)\n","    print(\"Packages installed successfully\")\n","except Exception as e:\n","    print(f\"Error installing packages: {e}\")\n","\n","# Imports para mÃ©tricas\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","import sacrebleu\n","from difflib import SequenceMatcher\n","\n","# Google Drive imports\n","from google.colab import auth\n","from googleapiclient.discovery import build\n","from googleapiclient.http import MediaIoBaseDownload\n","from googleapiclient.errors import HttpError\n","\n","# Descargar recursos de NLTK\n","nltk.download('punkt', quiet=True)\n","\n","# ============================================================================\n","# 1. AUTENTICAR Y CONFIGURAR GOOGLE DRIVE\n","# ============================================================================\n","\n","print(\"Authenticating with Google Drive...\")\n","auth.authenticate_user()\n","drive_service = build('drive', 'v3')\n","print(\"âœ“ Authenticated successfully\\n\")\n","\n","# ============================================================================\n","# 2. FUNCIÃ“N PARA DESCARGAR ARCHIVOS DESDE GOOGLE DRIVE\n","# ============================================================================\n","\n","def download_file_from_drive(file_id, file_name):\n","    \"\"\"Descarga un archivo de Google Drive y retorna su contenido como string.\"\"\"\n","    try:\n","        request = drive_service.files().get_media(fileId=file_id)\n","        fh = io.BytesIO()\n","        downloader = MediaIoBaseDownload(fh, request)\n","        done = False\n","\n","        print(f\"Downloading {file_name}...\", end=\" \")\n","        while done is False:\n","            status, done = downloader.next_chunk()\n","\n","        # Mover cursor al inicio y decodificar\n","        fh.seek(0)\n","        content = fh.read().decode(\"utf-8\")\n","        print(\"âœ“\")\n","        return content\n","\n","    except HttpError as error:\n","        print(f\"âœ— Error: {error}\")\n","        return None\n","\n","# ============================================================================\n","# 3. DESCARGAR ARCHIVOS CON LOS IDs CORRECTOS\n","# ============================================================================\n","\n","# IDs de los archivos\n","generated_outputs_file_id = \"1CQx7tABaPHvVrUQgPiy2TIoV60CFsqL0\"  # generated_tmf_format_outputs.json\n","eval_dataset_file_id = \"1-3umMuALfDG2jwYR443oOrufTQkpUA1s\"  # RMI_Intentions_Test_Dataset.json\n","\n","print(\"=\"*60)\n","print(\"LOADING FILES FROM GOOGLE DRIVE\")\n","print(\"=\"*60)\n","\n","# Descargar outputs generados\n","print(\"\\nDownloading generated outputs...\")\n","generated_content = download_file_from_drive(generated_outputs_file_id, \"generated_tmf_format_outputs.json\")\n","if generated_content:\n","    try:\n","        tmf_format_outputs = json.loads(generated_content)\n","        print(f\"  âœ“ Loaded {len(tmf_format_outputs)} generated outputs\")\n","    except json.JSONDecodeError as e:\n","        print(f\"  âœ— Error parsing generated outputs JSON: {e}\")\n","        raise\n","else:\n","    print(\"  âœ— Failed to load generated outputs\")\n","    raise Exception(\"Cannot continue without generated outputs\")\n","\n","# Descargar dataset de evaluaciÃ³n\n","print(\"\\nDownloading evaluation dataset...\")\n","eval_content = download_file_from_drive(eval_dataset_file_id, \"RMI_Intentions_Test_Dataset.json\")\n","if eval_content:\n","    try:\n","        eval_json_dataset = json.loads(eval_content)\n","        print(f\"  âœ“ Loaded evaluation dataset\")\n","        print(f\"  Dataset structure: {type(eval_json_dataset)}\")\n","        if isinstance(eval_json_dataset, dict):\n","            print(f\"  Dataset keys: {list(eval_json_dataset.keys())}\")\n","    except json.JSONDecodeError as e:\n","        print(f\"  âœ— Error parsing evaluation dataset JSON: {e}\")\n","        raise\n","else:\n","    print(\"  âœ— Failed to load evaluation dataset\")\n","    raise Exception(\"Cannot continue without evaluation dataset\")\n","\n","# ============================================================================\n","# 4. EXTRAER RESPUESTAS DE REFERENCIA (ESTRUCTURA CORREGIDA)\n","# ============================================================================\n","\n","print(\"\\nExtracting reference answers...\")\n","reference_outputs = []\n","\n","# El dataset tiene la estructura: {\"conversations\": [[{msg1}, {msg2}], [{msg1}, {msg2}], ...]}\n","# Cada elemento en la lista es una conversaciÃ³n completa\n","if isinstance(eval_json_dataset, dict) and 'conversations' in eval_json_dataset:\n","    conversations_list = eval_json_dataset['conversations']\n","    print(f\"  Found {len(conversations_list)} conversations\")\n","\n","    for i, conversation in enumerate(conversations_list):\n","        # Cada conversaciÃ³n es una lista de mensajes\n","        for message in conversation:\n","            if message.get('from') == 'model_answer':\n","                # Extraer el texto dentro de los triple backticks\n","                answer = message.get('value', '')\n","\n","                # Buscar contenido dentro de triple backticks usando regex\n","                import re\n","                pattern = re.search(r\"```(.*?)```\", answer, re.DOTALL)\n","                if pattern:\n","                    extracted_text = pattern.group(1).strip()\n","                    reference_outputs.append(extracted_text)\n","                else:\n","                    # Si no hay backticks, usar el valor completo\n","                    reference_outputs.append(answer)\n","                break  # Solo tomar la primera respuesta del modelo en la conversaciÃ³n\n","\n","    print(f\"  âœ“ Extracted {len(reference_outputs)} reference answers\")\n","else:\n","    print(\"  âœ— Unexpected dataset structure\")\n","    raise Exception(\"Dataset does not have expected 'conversations' key\")\n","\n","# Verificar que tengamos el mismo nÃºmero de outputs y referencias\n","print(f\"\\nData alignment check:\")\n","print(f\"  Generated outputs: {len(tmf_format_outputs)}\")\n","print(f\"  Reference answers: {len(reference_outputs)}\")\n","\n","if len(tmf_format_outputs) != len(reference_outputs):\n","    print(f\"  âš  WARNING: Different counts - adjusting to minimum\")\n","    min_len = min(len(tmf_format_outputs), len(reference_outputs))\n","    tmf_format_outputs = tmf_format_outputs[:min_len]\n","    reference_outputs = reference_outputs[:min_len]\n","    print(f\"  âœ“ Adjusted to {min_len} samples\")\n","else:\n","    print(f\"  âœ“ Counts match perfectly\")\n","\n","# ============================================================================\n","# 5. CALCULAR BLEU SCORE\n","# ============================================================================\n","\n","def calculate_bleu_scores(generated_outputs, reference_outputs):\n","    \"\"\"Calcula BLEU score para cada par de salidas generadas y referencias.\"\"\"\n","    bleu_scores = []\n","    smooth = SmoothingFunction()\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"CALCULATING BLEU SCORES...\")\n","    print(\"=\"*60)\n","\n","    for i, (generated, reference) in enumerate(zip(generated_outputs, reference_outputs)):\n","        # Manejar casos donde generated o reference puedan estar vacÃ­os\n","        if not generated or not reference:\n","            print(f\"\\nâš  Sample {i+1}: Empty generated or reference text, skipping...\")\n","            bleu_scores.append(0.0)\n","            continue\n","\n","        # Tokenizar\n","        generated_tokens = generated.split()\n","        reference_tokens = [reference.split()]\n","\n","        # Calcular BLEU con suavizado\n","        score = sentence_bleu(\n","            reference_tokens,\n","            generated_tokens,\n","            smoothing_function=smooth.method1\n","        )\n","        bleu_scores.append(score)\n","\n","        if i < 3:  # Mostrar primeros 3 ejemplos\n","            print(f\"\\nSample {i+1}:\")\n","            print(f\"  Generated (first 150 chars): {generated[:150]}...\")\n","            print(f\"  Reference (first 150 chars): {reference[:150]}...\")\n","            print(f\"  BLEU Score: {score:.4f}\")\n","\n","    # Calcular corpus BLEU usando sacrebleu\n","    corpus_bleu = sacrebleu.corpus_bleu(generated_outputs, [reference_outputs])\n","\n","    results = {\n","        'individual_scores': bleu_scores,\n","        'average_bleu': np.mean(bleu_scores),\n","        'corpus_bleu': corpus_bleu.score,\n","        'min_bleu': np.min(bleu_scores),\n","        'max_bleu': np.max(bleu_scores),\n","        'std_bleu': np.std(bleu_scores)\n","    }\n","\n","    print(\"\\n\" + \"-\"*60)\n","    print(\"BLEU SCORE RESULTS:\")\n","    print(\"-\"*60)\n","    print(f\"Average BLEU:     {results['average_bleu']:.4f}\")\n","    print(f\"Corpus BLEU:      {results['corpus_bleu']:.4f}\")\n","    print(f\"Min BLEU:         {results['min_bleu']:.4f}\")\n","    print(f\"Max BLEU:         {results['max_bleu']:.4f}\")\n","    print(f\"Std Deviation:    {results['std_bleu']:.4f}\")\n","    print(\"-\"*60)\n","\n","    return results\n","\n","# Calcular BLEU\n","bleu_results = calculate_bleu_scores(tmf_format_outputs, reference_outputs)\n","\n","# ============================================================================\n","# 6. CALCULAR ROC-AUC (BASADO EN SIMILITUD)\n","# ============================================================================\n","\n","def calculate_roc_auc_similarity(generated_outputs, reference_outputs):\n","    \"\"\"Calcula ROC-AUC usando similitud de texto como proxy.\"\"\"\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"CALCULATING ROC-AUC (SIMILARITY-BASED)...\")\n","    print(\"=\"*60)\n","    print(\"Note: ROC-AUC adapted for text generation using similarity scores\")\n","\n","    # Calcular similitudes\n","    similarities = []\n","    y_true = []\n","\n","    for i, (gen, ref) in enumerate(zip(generated_outputs, reference_outputs)):\n","        if not gen or not ref:\n","            similarities.append(0.0)\n","            y_true.append(1)\n","            continue\n","\n","        similarity = SequenceMatcher(None, gen, ref).ratio()\n","        similarities.append(similarity)\n","        y_true.append(1)  # Siempre 1 (positivo) para la referencia correcta\n","\n","        if i < 3:\n","            print(f\"\\nSample {i+1} Similarity: {similarity:.4f}\")\n","\n","    # Crear predicciones binarias basadas en threshold\n","    threshold = 0.5\n","    y_pred_binary = [1 if s >= threshold else 0 for s in similarities]\n","\n","    # Calcular mÃ©tricas\n","    try:\n","        # ROC-AUC usando similitudes como scores\n","        roc_auc = roc_auc_score(y_true, similarities)\n","\n","        # Calcular curva ROC\n","        fpr, tpr, thresholds = roc_curve(y_true, similarities)\n","\n","        # Graficar curva ROC\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(fpr, tpr, color='darkorange', lw=2,\n","                 label=f'ROC curve (AUC = {roc_auc:.4f})')\n","        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n","        plt.xlim([0.0, 1.0])\n","        plt.ylim([0.0, 1.05])\n","        plt.xlabel('False Positive Rate', fontsize=12)\n","        plt.ylabel('True Positive Rate', fontsize=12)\n","        plt.title('ROC Curve - Text Similarity Classification', fontsize=14, fontweight='bold')\n","        plt.legend(loc=\"lower right\", fontsize=10)\n","        plt.grid(True, alpha=0.3)\n","        plt.tight_layout()\n","        plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n","        print(\"\\nâœ“ ROC curve saved as 'roc_curve.png'\")\n","        plt.close()\n","\n","        # Calcular accuracy basada en threshold\n","        accuracy = sum([1 for p, t in zip(y_pred_binary, y_true) if p == t]) / len(y_true)\n","\n","        results = {\n","            'roc_auc': roc_auc,\n","            'average_similarity': np.mean(similarities),\n","            'min_similarity': np.min(similarities),\n","            'max_similarity': np.max(similarities),\n","            'std_similarity': np.std(similarities),\n","            'accuracy_at_threshold': accuracy,\n","            'threshold_used': threshold,\n","            'individual_similarities': similarities\n","        }\n","\n","        print(\"\\n\" + \"-\"*60)\n","        print(\"ROC-AUC RESULTS:\")\n","        print(\"-\"*60)\n","        print(f\"ROC-AUC Score:           {roc_auc:.4f}\")\n","        print(f\"Average Similarity:      {results['average_similarity']:.4f}\")\n","        print(f\"Min Similarity:          {results['min_similarity']:.4f}\")\n","        print(f\"Max Similarity:          {results['max_similarity']:.4f}\")\n","        print(f\"Std Similarity:          {results['std_similarity']:.4f}\")\n","        print(f\"Accuracy (threshold={threshold}): {accuracy:.4f}\")\n","        print(\"-\"*60)\n","\n","        return results\n","\n","    except Exception as e:\n","        print(f\"âœ— Error calculating ROC-AUC: {e}\")\n","        return None\n","\n","# Calcular ROC-AUC\n","roc_results = calculate_roc_auc_similarity(tmf_format_outputs, reference_outputs)\n","\n","# ============================================================================\n","# 7. GUARDAR RESULTADOS\n","# ============================================================================\n","\n","from datetime import datetime\n","\n","# Timestamp\n","now = datetime.now()\n","timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","# Crear diccionario con todos los resultados\n","metrics_results = {\n","    'timestamp': timestamp,\n","    'total_samples': len(tmf_format_outputs),\n","    'bleu_metrics': {\n","        'average_bleu': float(bleu_results['average_bleu']),\n","        'corpus_bleu': float(bleu_results['corpus_bleu']),\n","        'min_bleu': float(bleu_results['min_bleu']),\n","        'max_bleu': float(bleu_results['max_bleu']),\n","        'std_bleu': float(bleu_results['std_bleu']),\n","        'individual_scores': [float(s) for s in bleu_results['individual_scores']]\n","    },\n","    'roc_auc_metrics': {\n","        'roc_auc': float(roc_results['roc_auc']) if roc_results else None,\n","        'average_similarity': float(roc_results['average_similarity']) if roc_results else None,\n","        'min_similarity': float(roc_results['min_similarity']) if roc_results else None,\n","        'max_similarity': float(roc_results['max_similarity']) if roc_results else None,\n","        'std_similarity': float(roc_results['std_similarity']) if roc_results else None,\n","        'accuracy_at_threshold': float(roc_results['accuracy_at_threshold']) if roc_results else None,\n","        'individual_similarities': [float(s) for s in roc_results['individual_similarities']] if roc_results else []\n","    }\n","}\n","\n","# Guardar como JSON localmente\n","output_filename = f'metrics_results_{timestamp}.json'\n","with open(output_filename, 'w', encoding='utf-8') as f:\n","    json.dump(metrics_results, f, indent=2, ensure_ascii=False)\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"RESULTS SAVED LOCALLY\")\n","print(\"=\"*60)\n","print(f\"âœ“ Metrics saved to: {output_filename}\")\n","print(f\"âœ“ ROC curve saved to: roc_curve.png\")\n","print(\"=\"*60)\n","\n","# Resumen final\n","print(\"\\n\" + \"=\"*60)\n","print(\"EVALUATION SUMMARY\")\n","print(\"=\"*60)\n","print(f\"Total samples evaluated: {len(tmf_format_outputs)}\")\n","print(f\"\\nBLEU Metrics:\")\n","print(f\"  Average BLEU Score: {bleu_results['average_bleu']:.4f}\")\n","print(f\"  Corpus BLEU Score:  {bleu_results['corpus_bleu']:.4f}\")\n","if roc_results:\n","    print(f\"\\nROC-AUC Metrics:\")\n","    print(f\"  ROC-AUC Score:      {roc_results['roc_auc']:.4f}\")\n","    print(f\"  Avg Similarity:     {roc_results['average_similarity']:.4f}\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NekDSxUUIJS-","executionInfo":{"status":"ok","timestamp":1759463952908,"user_tz":300,"elapsed":8781,"user":{"displayName":"ANGEL ROBLEDO GIRON","userId":"02782444304581830579"}},"outputId":"19ac02e2-4a63-405b-e0df-a0d8c85f98a7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Packages installed successfully\n","Authenticating with Google Drive...\n","âœ“ Authenticated successfully\n","\n","============================================================\n","LOADING FILES FROM GOOGLE DRIVE\n","============================================================\n","\n","Downloading generated outputs...\n","Downloading generated_tmf_format_outputs.json... âœ“\n","  âœ“ Loaded 15 generated outputs\n","\n","Downloading evaluation dataset...\n","Downloading RMI_Intentions_Test_Dataset.json... âœ“\n","  âœ“ Loaded evaluation dataset\n","  Dataset structure: <class 'dict'>\n","  Dataset keys: ['conversations']\n","\n","Extracting reference answers...\n","  Found 10 conversations\n","  âœ“ Extracted 10 reference answers\n","\n","Data alignment check:\n","  Generated outputs: 15\n","  Reference answers: 10\n","  âš  WARNING: Different counts - adjusting to minimum\n","  âœ“ Adjusted to 10 samples\n","\n","============================================================\n","CALCULATING BLEU SCORES...\n","============================================================\n","\n","âš  Sample 1: Empty generated or reference text, skipping...\n","\n","âš  Sample 2: Empty generated or reference text, skipping...\n","\n","âš  Sample 3: Empty generated or reference text, skipping...\n","\n","âš  Sample 4: Empty generated or reference text, skipping...\n","\n","âš  Sample 5: Empty generated or reference text, skipping...\n","\n","âš  Sample 8: Empty generated or reference text, skipping...\n","\n","âš  Sample 9: Empty generated or reference text, skipping...\n","\n","âš  Sample 10: Empty generated or reference text, skipping...\n","\n","------------------------------------------------------------\n","BLEU SCORE RESULTS:\n","------------------------------------------------------------\n","Average BLEU:     0.0227\n","Corpus BLEU:      0.0364\n","Min BLEU:         0.0000\n","Max BLEU:         0.1320\n","Std Deviation:    0.0461\n","------------------------------------------------------------\n","\n","============================================================\n","CALCULATING ROC-AUC (SIMILARITY-BASED)...\n","============================================================\n","Note: ROC-AUC adapted for text generation using similarity scores\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1179: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ“ ROC curve saved as 'roc_curve.png'\n","\n","------------------------------------------------------------\n","ROC-AUC RESULTS:\n","------------------------------------------------------------\n","ROC-AUC Score:           nan\n","Average Similarity:      0.0458\n","Min Similarity:          0.0000\n","Max Similarity:          0.2664\n","Std Similarity:          0.0931\n","Accuracy (threshold=0.5): 0.0000\n","------------------------------------------------------------\n","\n","============================================================\n","RESULTS SAVED LOCALLY\n","============================================================\n","âœ“ Metrics saved to: metrics_results_2025-10-03_03-59-12.json\n","âœ“ ROC curve saved to: roc_curve.png\n","============================================================\n","\n","============================================================\n","EVALUATION SUMMARY\n","============================================================\n","Total samples evaluated: 10\n","\n","BLEU Metrics:\n","  Average BLEU Score: 0.0227\n","  Corpus BLEU Score:  0.0364\n","\n","ROC-AUC Metrics:\n","  ROC-AUC Score:      nan\n","  Avg Similarity:     0.0458\n","============================================================\n"]}]}]}
